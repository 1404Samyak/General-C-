1)C++_Basics:
  Syntax:
    Function_Declaration: "return_type function_name(parameters) { /* code */ }"
    If_Else: |
      if (condition) {
        // code
      } else {
        // code
      }
    Loops:
      For: "for (int i = 0; i < n; i++)"
      While: "while (condition) { }"
      Do_While: "do { } while (condition);"

  Pointers_And_References:
    Pointer:
      Declaration: "int* p = &x;"
      Meaning: "'p' stores address of 'x'"
      Dereference: "*p gives value at the address"
    Reference:
      Declaration: "int& ref = x;"
      Meaning: "'ref' is another name for 'x', changes reflect both ways"

  Comparators:
    Custom_Sort_Vector:
      Code: |
        bool cmp(pair<int,int> a, pair<int,int> b) {
          return a.first > b.first; // descending
        }
        sort(vec.begin(), vec.end(), cmp);
      Use: "Custom sorting logic in STL algorithms"

  Vector:
    Declaration: "vector<int> v;"
    Common_Operations:
      Push_Back: "v.push_back(x);"
      Access: "v[i], v.at(i)"
      Size: "v.size()"
      Back: "v.back()"
      Front: "v.front()"
      Pop_Back: "v.pop_back();"
      Clear: "v.clear();"

  Stack:
    Header: "#include<stack>"
    Operations:
      Declaration: "stack<int> s;"
      Push: "s.push(x);"
      Pop: "s.pop();"
      Top: "s.top();"
      Size: "s.size();"
      Empty: "s.empty();"

  Queue:
    Header: "#include<queue>"
    Operations:
      Declaration: "queue<int> q;"
      Push: "q.push(x);"
      Pop: "q.pop();"
      Front: "q.front();"
      Back: "q.back();"
      Size: "q.size();"
      Empty: "q.empty();"

  Priority_Queue:
    Max_Heap: "priority_queue<int> pq;"
    Min_Heap: |
      priority_queue<int, vector<int>, greater<int>> pq;
    Custom_Comparator: |
      struct cmp {
        bool operator()(pair<int,int>& a, pair<int,int>& b) {
          return a.second > b.second;
        }
      };
      priority_queue<pair<int,int>, vector<pair<int,int>>, cmp> pq;

  Nested_Loops:
    Basic_Syntax: |
      for(int i = 0; i < n; i++) {
        for(int j = 0; j < m; j++) {
          // code using i, j
        }
      }
    Use_Cases:
      Matrix_Traversal: "For 2D matrix/grid problems"
      Subarray_Sum: |
        for(int i = 0; i < n; i++) {
          int sum = 0;
          for(int j = i; j < n; j++) {
            sum += arr[j];
            // process sum(i..j)
          }
        }
      Adjacency_Checking: |
        for(int u = 0; u < n; u++) {
          for(auto v : adj[u]) {
            // process edge u -> v
          }
        }
      Prefix_Sum_2D: |
        for(int i = 0; i < n; i++)
          for(int j = 0; j < m; j++)
            sum[i][j] = A[i][j] + sum[i-1][j] + sum[i][j-1] - sum[i-1][j-1];

  Complex_Declarations:
    Adjacency_List:
      Undirected_Graph: "vector<vector<int>> adj(n);"
      Weighted_Graph: "vector<vector<pair<int,int>>> adj(n); // {v, wt}"
      3D_Adjacency: "vector<vector<vector<pair<int,int>>>> graph;"
    Vector_of_Pairs: "vector<pair<int,int>> vp;"
    Vector_of_Vector_of_Pairs: "vector<vector<pair<int,int>>> vvp;"
    Set_of_Pairs: "set<pair<int,int>> s;"
    Map:
      Frequency_Map: "map<int,int> freq;"
      Char_Map: "unordered_map<char,int> m;"

  Notes:
    STL_Usage:
      - "Always use pass-by-reference in loops: for(auto &x : vec)"
      - "Vectors are dynamic, arrays are static"
    Tips:
      - "Prefer vector over array for flexibility"
      - "Use nested loops for pairwise comparisons, prefix/suffix logic, grid traversal"

2)Check_Valid_Parentheses:
  Purpose: "Check if a given string of parentheses is valid (balanced and properly nested)."

  Functions:
    match:
      Signature: "bool match(char ch1, char ch2)"
      Purpose: "Checks if the two characters form a valid opening-closing bracket pair."
      Logic:
        - "'(' matches with ')'"
        - "'{' matches with '}'"
        - "'[' matches with ']'"
      Return: "true if the pair is valid, else false"

    checkValidString:
      Signature: "bool checkValidString(string s)"
      Purpose: "Check if the input string `s` has valid and balanced brackets using a stack."

      Steps:
        1. Initialize a stack:
          Code: "stack<char> st;"
          Reason: "To keep track of unmatched opening brackets"
        
        2. Loop through each character in the string:
          Loop: "for (int i = 0; i < n; i++)"

          Cases:
            - If current character is a closing bracket: ')', '}', or ']':
                Conditions:
                  - If stack is empty:
                      Meaning: "No opening bracket to match with this closing bracket"
                      Action: "return false"
                  - Else, check if top of stack matches:
                      Logic: "match(st.top(), s[i])"
                      If true: "Pop from stack (valid pair found)"
                      Else: "return false (mismatched brackets)"
            
            - Else: (opening bracket):
                Action: "Push it onto the stack"
                Reason: "To wait for its corresponding closing bracket later"

        3. Final Stack Check:
          Code: "return st.empty();"
          Meaning: 
            -After the for loop is processed fully there is chance opening brackets may be left,for sure Only opening brackets are possible to be leftbehind not closing brackets because all closing brackets we are processig inside the for loop only.So if any opening brackets are left then also retur false,else return true
            - "If stack is empty: All brackets matched properly → return true"
            - "If stack is not empty: Some opening brackets were not closed → return false"

  Example_Run:
    Input: "({[]})"
    Stack_Trace:
      - Push '('
      - Push '{'
      - Push '['
      - Match '[' and ']' → pop
      - Match '{' and '}' → pop
      - Match '(' and ')' → pop
    Final_Stack: "empty → return true"

  Edge_Cases:
    - Input: "([)]"
      Explanation: "Although brackets are balanced in number, nesting is invalid → return false"
    - Input: "(()"
      Explanation: "Missing closing bracket for one opening → return false"
    - Input: "" (empty string)
      Explanation: "Technically balanced → return true"

  Notes:
    - Time_Complexity: "O(n)"
    - Space_Complexity: "O(n) in worst case (all openings)"
    - Only works for '(', ')', '{', '}', '[', ']' — doesn't support wildcards like '*'

  Optional_Extensions:
    - Support wildcard '*' as either '(' or ')' or empty (like in Leetcode problem)
    - Count total matched pairs
    - Return position of error for debugging

3)Valid_Parenthesis_With_Star:
  Problem:
    Statement: |
      Check whether a given string with characters '(', ')', and '*' is valid.
      '*' can represent either '(', ')' or an empty string.
    Goal: |
      Return true if the string can be interpreted as a valid sequence of parentheses.

  Core_Logic:
    Stacks_Used:
      open:
        Description: "Stores indices of unmatched '(' characters"
      star:
        Description: "Stores indices of '*' characters (can act as '(', ')' or empty)"
    
    Main_Loop:
      Iterate_Over_String:
        Code: |
          for (int i = 0; i < n; i++) {
              if (s[i] == '(') open.push(i);
              else if (s[i] == '*') star.push(i);
              else {
                  if (!open.empty()) open.pop();
                  else if (!star.empty()) star.pop();
                  else return false;
              }
          }
        Purpose:
          - Push index of '(' to open stack
          - Push index of '*' to star stack
          - When ')' is encountered:
              - First try to match with a previous '('
              - Else use '*' as '('
              - Else return false (no matching opening)
    
    Post_Loop_Compensation:
      Why_Needed: |
        After the loop, unmatched '(' may still remain in the open stack.
        These need to be matched using '*' treated as ')'.
        But only if '*' appeared **after** the '(' in the string.
      Code: |
        while (!open.empty() && !star.empty()) {
            if (open.top() > star.top()) return false;
            open.pop();
            star.pop();
        }
      Reasoning:
        - '(' at a later index than '*' means '*' appeared too early to act as ')'
        - So such combinations are invalid

    Final_Return:
      Code: |
        return open.empty();
      Meaning:
        - If all '(' were matched (directly or via '*'), return true
        - If any unmatched '(', return false

  Dry_Run_Example:
    Input: "(*))"
    Stack_States:
      Step_1: '(' at 0 → open = [0]
      Step_2: '*' at 1 → star = [1]
      Step_3: ')' at 2 → match with '(' → open.pop()
      Step_4: ')' at 3 → no '(', match with '*' → star.pop()
    Final: both stacks empty → ✅ valid

    Input: "*)("
    Stack_States:
      Step_1: '*' at 0 → star = [0]
      Step_2: ')' at 1 → no '(', match with '*' → star.pop()
      Step_3: '(' at 2 → open = [2]
    Final: open = [2], star = [] → ❌ invalid

  Time_Complexity: "O(n)"
  Space_Complexity: "O(n)"
  Constraints:
    - Only supports '(', ')', and '*'
    - Does not support '{' or '[' types of brackets

  Notes:
    - Clever use of indices allows enforcing correct ordering
    - The problem is based on Leetcode 678: Valid Parenthesis String
    - This greedy two-stack method is more efficient than backtracking all '*' cases

4)problem: Maximum Number of Non-Overlapping Meetings
constraints:
  - You are given two arrays: start[i], end[i] of size n
  - You must select the maximum number of meetings such that:
    - No two selected meetings overlap
    - The start time of a meeting must be strictly greater than the end time of the last selected meeting

solution_type: Greedy Algorithm

key_observation:
  - To schedule the maximum number of non-overlapping meetings, you should always choose the meeting that ends the earliest
  - This leaves the most room for future meetings

why_sort_by_end_time:
  - If you sort by end time, you always consider the meeting that finishes earliest first
  - This minimizes the time blocked by the current meeting
  - It ensures you leave space for as many future meetings as possible
  - This is optimal because picking a later-ending meeting might block a lot of shorter earlier-starting meetings

comparator_used:
  syntax: |
    sort(v.begin(), v.end(), [](pair<int, int>& p1, pair<int, int>& p2) {
        return p1.second <= p2.second;
    });

  explanation:
    - For two pairs p1 and p2 representing (start, end) of meetings:
      - If p1.second < p2.second → p1 comes before p2 ✅
      - If p1.second == p2.second → p1 comes before p2 (allowed because <= is true)
      - No explicit tie-breaking is used, but it doesn't matter for greedy correctness
    - This sort places meetings with the earliest end times at the front
    - Even if some meetings have the same end time, any one of them is safe to consider first

greedy_selection:
  - Initialize endtime = -infinity (or 0)
  - Traverse the sorted meetings
  - For each meeting:
      if start > endtime:
        - select the meeting
        - update endtime = current meeting's end
  - This ensures strictly non-overlapping selection

correctness_reasoning:
  - Greedy always picks the next meeting that ends the soonest
  - Once picked, no earlier-ending meeting could have been selected (because you already processed them)
  - By always choosing the best local (earliest ending) option, the global (maximum count) is achieved
  - This has been proven correct for the interval scheduling maximization problem

time_complexity:
  sorting: O(n log n)
  greedy_selection: O(n)
  total: O(n log n)

space_complexity: O(n) due to pair storage, O(1) if in-place sorting used

note:
  - If problem allows start == end, use '>='
  - If problem requires strict non-overlap (start > end), use '>'

5)problem: Merge Overlapping Intervals
description: |
  Given a list of intervals where each interval is a pair [start, end],
  merge all overlapping intervals and return a list of non-overlapping intervals
  that cover all the input intervals.

approach: Greedy Merge

sorting_step:
  reason: |
    To easily detect overlapping intervals, we must process them in order of their start times.
    Sorting by start time ensures all overlapping intervals are adjacent in the list.
  code_snippet: |
    sort(intervals.begin(), intervals.end());

variables:
  - n: number of intervals
  - ans: list to store the merged, non-overlapping intervals

main_loop:
  iteration: for each interval in intervals
  logic:
    - if ans is empty:
        - this is the first interval, add it to ans
    - else if current interval does NOT overlap with last interval in ans:
        - check: interval[0] > ans.back()[1]
        - reasoning: if current start > last end, no overlap exists
        - action: push current interval into ans
    - else if current interval overlaps with last interval in ans:
        - merge by updating end time
        - new_end = max(current_end, ans.back()[1])

  code_snippet: |
    for (auto interval : intervals) {
        if (ans.empty())
            ans.push_back(interval);
        else if (interval[0] > ans.back()[1])
            ans.push_back(interval);
        else
            ans.back()[1] = max(ans.back()[1], interval[1]);
    }

final_return:
  - return the list of merged intervals
  code_snippet: |
    return ans;

example_dry_run:
  input: [[1,3], [2,6], [8,10], [15,18]]
  sorted: [[1,3], [2,6], [8,10], [15,18]]
  steps:
    - [1,3] → ans = [[1,3]]
    - [2,6] overlaps → merge → ans = [[1,6]]
    - [8,10] no overlap → ans = [[1,6], [8,10]]
    - [15,18] no overlap → ans = [[1,6], [8,10], [15,18]]
  output: [[1,6], [8,10], [15,18]]

time_complexity:
  sorting: O(n log n)
  merge_loop: O(n)
  total: O(n log n)

space_complexity:
  worst_case: O(n) for answer list if no intervals overlap

why_this_works:
  - Sorting ensures overlapping intervals are adjacent
  - Greedy merge ensures only necessary intervals are merged
  - We maintain the latest end time seen so far to check overlaps

notes:
  - Works only if intervals are sorted by start time
  - Using `ans.back()` allows constant-time access to last merged interval

6)maxSumCombinations:
  purpose: >
    To find the k largest sum combinations from two arrays nums1 and nums2,
    where each sum is formed by adding one element from nums1 and one from nums2.

  key_idea:
    - Sort both arrays in descending order to prioritize larger elements first.
    - Use a max-heap (priority_queue) to always extract the next largest combination.
    - Avoid duplicates by using a set to track visited index pairs (i, j).
    - Push neighboring combinations of the current pair to explore all possible top combinations.

  why_insert_both_neighbors:
    reason_1: "We are interested in the top-k largest sums — not just the next immediate maximum."
    reason_2: >
      Even if (i+1, j) has a greater sum than (i, j+1) now,
      the path from (i, j+1) might lead to even larger combinations later.
      Ignoring it could miss some of the actual top-k combinations.
    reason_3: >
      Both directions (i+1, j) and (i, j+1) represent different combination paths.
      Only pushing one of them can miss combinations involving nums1[i] with later nums2[j+1],
      or nums2[j] with later nums1[i+1].
    reason_4: >
      The max-heap takes care of ordering and selecting the next largest sum,
      so it's safe and optimal to explore both neighbors and let the heap manage which one to pick next.

  role_of_set:
    purpose: "To prevent visiting the same (i, j) index pair multiple times."
    ensures:
      - No duplicate sums are pushed into the heap.
      - Memory and computation are saved.
      - Correctness: prevents redundant computation and infinite loop.
    inserted_when:
      - After pushing (i+1, j) into pq, insert (i+1, j) into the set.
      - After pushing (i, j+1) into pq, insert (i, j+1) into the set.

  heap_invariant:
    - The max-heap always stores the current best unseen combinations.
    - It ensures that at each step, the largest possible sum is picked greedily.
    - By pushing both neighbors, the algorithm maintains correctness and completeness.

  correctness:
    - The algorithm guarantees that the top-k combinations are extracted in non-increasing order.
    - No combination is revisited.
    - All necessary potential candidates are explored due to the neighbor insertions.

  time_and_space_complexity:
    time: "O(k * log k + n log n) — dominated by heap operations and initial sorting"
    space: "O(k) for heap and visited set"

  conclusion: >
    Always push both neighbors (i+1, j) and (i, j+1) if they are within bounds and not visited.
    This ensures that no potential top-k sum is missed, and that the heap has all possible candidates to work with.

7)Minimum Cost to Connect Sticks:
  problem_statement: >
    You are given an array `sticks`, where each element represents the length of a stick.
    You want to connect all the sticks into one stick.
    The cost of connecting two sticks is equal to the sum of their lengths.
    Your goal is to connect the sticks such that the **total cost is minimized**.

  key_observation: >
    Always connect the two shortest sticks first to minimize the cost.
    This is a greedy strategy similar to Huffman coding.

  why_min_heap:
    reason: >
      To always get the two smallest sticks in O(log n) time efficiently,
      we use a **priority queue (min-heap)**. The top element of a min-heap is the smallest.
    min_heap_behavior:
      - Insertion (push): O(log n)
      - Deletion (pop): O(log n)
      - Access minimum: O(1)

  approach:
    data_structure: min-heap (priority_queue with greater<int>)
    steps:
      - Step 1: Insert all stick lengths into the min-heap.
      - Step 2: While more than one stick remains:
          - Pop the two smallest sticks from the heap.
          - Compute their sum (cost of connecting them).
          - Add the cost to the answer.
          - Push the new stick (sum) back into the heap.
      - Step 3: Return the accumulated cost.

  dry_run_example:
    input: [2, 4, 3]
    heap_states:
      - Initial heap: [2, 4, 3]
      - Pop 2 and 3 → cost = 5 → heap: [4, 5]
      - Pop 4 and 5 → cost = 9 → heap: [9]
    total_cost: 5 + 9 = 14

  time_complexity:
    expression: O(n log n)
    explanation: >
      Each insertion and removal in the heap is O(log n),
      and you perform about n−1 such operations.

  space_complexity:
    expression: O(n)
    explanation: >
      The heap stores all n sticks in the worst case.

  code_snippet: |
        int connectSticks(vector<int>& sticks) {
            // Min-heap to always pick the two smallest sticks
            priority_queue<int, vector<int>, greater<int>> pq;
            // Push all stick lengths into the min-heap
            for (int stick : sticks) {
                pq.push(stick);
            }
            int totalCost = 0;
            // Keep combining the two smallest sticks until one stick remains
            while (pq.size() >= 2) {
                int a = pq.top(); pq.pop(); // Smallest stick
                int b = pq.top(); pq.pop(); // Second smallest stick
                int cost = a + b;
                totalCost += cost;
                pq.push(cost); // Insert the combined stick back
            }
            return totalCost;
        }
    };

8)Problem: Minimum Moves to Make All Array Elements Equal (by incrementing n-1 elements in each move)
Intuition:
  - After performing all the operations, every element in the array becomes equal to some final value `x`.
  - In one move, we can increase any `n-1` elements by 1. Hence, we are only allowed to increase elements — no decreasing is allowed.
  - For any element `a[i]`, the number of steps needed to become `x` is `x - a[i]`.
  - Therefore, the maximum of all `(x - a[i])` across the array will determine how many total moves we make.
  - Since x must be ≥ max(a), the effective number of moves becomes `x - min(a)`, because the smallest element needs the most increments.

Goal:
  - Find the minimum number of moves to make all elements equal, using the fact that only `n-1` elements can be incremented per move.

Approach:
  Step 1: Sum Increase Per Move
    - In each move, we increase `n - 1` elements by 1.
    - So the total sum of the array increases by `n - 1` after each move.

  Step 2: Number of Moves
    - Suppose we make `m` moves in total.
    - Let the smallest element in the array be `mini`.
    - We assume after all moves, every element becomes `x`, so:
        x = mini + m

  Step 3: Final and Initial Sum Relation
    - Let initial sum of array = `sum`
    - After m moves, total sum becomes:  
        sum + m * (n - 1)
    - But this must also equal `n * x`, because now all elements are `x`.

  Step 4: Form Equation and Solve
    Equation:
      n * x = sum + (x - mini) * (n - 1)
    Solve:
      Left side: n * x  
      Right side: sum + (x - mini)(n - 1)

    So: n * x - (x - mini)(n - 1) = sum
      => x [n - (n - 1)] + mini * (n - 1) = sum  
      => x + mini * (n - 1) = sum  
      => x = sum - mini * (n - 1)

  Step 5: Final Answer
    - We need to compute total number of moves, which is:
        x - mini = sum - mini * n

Code_Snippet_C++:
code: |
  int movesRequired(vector<int>& a) {
      int sum = accumulate(a.begin(), a.end(), 0);
      int mini = *min_element(a.begin(), a.end());
      int n = a.size();
      return sum - mini * n;
  }

9)Problem: Minimum Moves to Equal Array Elements II
Description:
  - You are given an array `nums` of size `n`.
  - In one move, you can increment or decrement any element by 1.
  - Each move costs 1 unit.
  - Goal: Find the minimum total number of moves to make all elements equal.

Intuition:
  - We aim to minimize the total cost defined as:
      sum(|nums[i] - x|) for some chosen target `x`.
  - This sum of absolute differences is minimized when `x` is the **median** of the array.

Approach_1: Prefix + Suffix Sum Method (User's Code)

Step-by-step:
  1. Sort the array `nums`.
  2. Compute prefix sums:
      - pf[i] = sum of first i+1 elements
  3. Compute suffix sums:
      - sf[i] = sum of elements from i to n-1
  4. For each index `i`, treat `nums[i]` as the target and compute:
      - left_cost = (i+1) * nums[i] - pf[i]
      - right_cost = sf[i] - (n-i) * nums[i]
      - total_cost = left_cost + right_cost
  5. Take the minimum over all such costs.

Time_Complexity: O(n log n)
Space_Complexity: O(n)

Code_Snippet_PrefixSuffix:
  code: |
    class Solution {
    public:
        int minMoves2(vector<int>& nums) {
            int n = nums.size();
            sort(nums.begin(), nums.end()); 
            vector<long long> pf(n), sf(n);
            pf[0] = nums[0];
            for(int i = 1; i < n; i++) {
                pf[i] = pf[i - 1] + nums[i];
            }
            sf[n - 1] = nums[n - 1];
            for(int i = n - 2; i >= 0; i--) {
                sf[i] = sf[i + 1] + nums[i];
            }
            long long ans = LLONG_MAX;
            for(int i = 0; i < n; i++) {
                long long left = (long long)(i + 1) * nums[i] - pf[i];
                long long right = sf[i] - (long long)(n - i) * nums[i];
                ans = min(ans, left + right);
            }
            return static_cast<int>(ans);
        }
    };

Approach_2: Median-Based Greedy Method (Optimal)
Concept:
  - The sum of absolute differences is minimized at the median of the array.
  - So, just compute the total cost of converting all elements to the median.

Steps:
  1. Sort the array.
  2. Choose the middle element as the median.
  3. Compute sum of absolute differences from the median.

Time_Complexity: O(n log n)
Space_Complexity: O(1)

Code_Snippet_Median:
  code: |
    class Solution {
    public:
        int minMoves2(vector<int>& nums) {
            sort(nums.begin(), nums.end());
            int median = nums[nums.size() / 2];
            int total = 0;
            for (int num : nums) {
                total += abs(num - median);
            }
            return total;
        }
    };
  
Comparison:
  - Prefix-Suffix Sum:
      - Detailed, more math-heavy, flexible for learning.
      - Useful if you want to compute cost for all targets.
  - Median-Based:
      - Shortest, most optimal in time and space.
      - Uses mathematical property of absolute sum minimization.

Final_Recommendation:
  - Use median-based method in production for simplicity and performance.
  - Keep prefix-suffix logic as a great way to understand cumulative cost computation.

10)Problem: Minimum Beans to Remove to Equalize Bags
Description:
  - You're given an array `beans[]` where each element represents the number of beans in a bag.
  - In one operation, you can remove **any number of beans** from any bag.
  - You must choose **one integer x ≥ 0**, and make all non-empty bags have exactly x beans.
  - Your goal is to **minimize the total number of beans removed**.

Intuition:
  - After sorting, for any index `i`, you can:
      - Remove **all beans in bags from index 0 to i-1** (i.e., make those bags empty).
      - Make all remaining bags (from i to n-1) have exactly `beans[i]` beans.
      - This means reducing all `beans[j]` where j ≥ i to `beans[i]`.

Approach: Prefix and Suffix Sum with Sorted Array

Steps:
  1. **Sort** the array to ensure increasing order.
  2. **Prefix Sum (pf)**:
      - `pf[i]` = total beans from index 0 to i
      - Helps in quickly calculating cost to remove all beans from bags `0` to `i-1`.
  3. **Suffix Sum (sf)**:
      - `sf[i]` = total beans from index i to n-1
      - Helps in calculating cost to reduce bags from `i` to `n-1` to `beans[i]`
  4. For each index `i`:
      - Set `beans[i]` as the target value.
      - Cost:
        - Remove all from 0 to i-1 → `pf[i-1]`
        - Reduce all from i to n-1 to `beans[i]` → `sf[i] - (n - i) * beans[i]`
      - Total cost = `pf[i-1] + sf[i] - (n - i) * beans[i]`
  5. Track the minimum total cost over all such `i`.

Edge_Case:
  - For `i = 0`, we never enter the loop, so we handle it outside:
    - Cost = remove all beans down to `beans[0]` → `sf[0] - n * beans[0]`

Time_Complexity: O(n log n)
  - Sorting: O(n log n)
  - Prefix/suffix + loop: O(n)

Space_Complexity: O(n)
  - For prefix and suffix arrays

Code_Snippet:
  code: |
    class Solution {
    public:
        typedef long long ll;
        long long minimumRemoval(vector<int>& beans) {
            int n = beans.size();
            sort(beans.begin(), beans.end());
            vector<ll> pf(n), sf(n); // prefix and suffix sums
            pf[0] = beans[0];
            for (int i = 1; i < n; i++) {
                pf[i] = pf[i - 1] + beans[i];
            }
            sf[n - 1] = beans[n - 1];
            for (int i = n - 2; i >= 0; i--) {
                sf[i] = sf[i + 1] + beans[i];
            }
            ll ans = LLONG_MAX;
            for (int i = 1; i < n; i++) {
                ll temp = pf[i - 1] + sf[i] - 1LL * (n - i) * beans[i];
                ans = min(ans, temp);
            }
            // Special case for i = 0 (all bags should have beans[0])
            ans = min(ans, sf[0] - 1LL * n * beans[0]);
            return ans;
        }
    };

Alternative_Optimal_Approach:
  - Instead of prefix/suffix, just use total sum:
    - total_sum = sum of all beans
    - For each i:
      - beans_kept = beans[i] * (n - i)
      - beans_removed = total_sum - beans_kept
  - This is faster and simpler with same time complexity.

11)Problem: Minimum Operations to Reduce x to Zero
Description:
  - You are given an array `nums` and an integer `x`.
  - You can remove elements from either **left end** or **right end** of the array.
  - The goal is to find the **minimum number of operations** (removals from either side) such that the **sum of removed elements = x**.
  - If not possible, return -1.

Intuition:
  - Removing from left = prefix sum
  - Removing from right = suffix sum
  - We want to remove elements from both ends such that the **total removed sum = x**
  - Equivalently, we want to **find a subarray in the middle** whose sum is equal to `total_sum - x` and **keep** it (so all other elements are removed).
  - But this implementation follows a different strategy:  
    **build prefix sums**, then **scan from right**, maintaining a suffix sum and checking if a corresponding prefix sum exists that complements the suffix to reach x.

Approach:

  Step 1: Preprocess Prefix Sum
    - Build a prefix sum array `pf[]` where:
        pf[i] = sum of nums[0] to nums[i]
    - Store all prefix sums and their **last occurring index** in a map `m`:
        m[pf[i]] = i

  Step 2: Initialize
    - Set `m[0] = -1` to handle the case where prefix sum starts from index 0.
    - `ans` is initialized to INT_MAX for tracking the minimum operations.

  Step 3: Check Direct Prefix Case
    - If there is a prefix sum directly equal to `x`:
        - The first `i+1` elements sum to `x`, so we may not need to take anything from the right.
        - Update `ans = m[x] + 1`

  Step 4: Iterate Backward to Build Suffix Sums
    - Start from the end of array.
    - For each index `j` from `n-1` to `0`:
        - Accumulate `sum` of suffix.
        - Compute `remaining = x - sum`
        - If `remaining` is present in prefix map `m` and `m[remaining] < j` (no overlap):
            - Total elements used = prefix length + suffix length = `m[remaining] + 1 + (n - j)`
            - Update `ans = min(ans, total)`

  Step 5: Final Result
    - If `ans` was updated, return it.
    - Otherwise, return -1.

Time_Complexity: O(n)
  - One pass for prefix sum
  - One backward pass for suffix sum
  - Constant-time lookups in map

Space_Complexity: O(n)
  - For prefix sum and hash map

Code_Snippet:
  code: |
    class Solution {
    public:
        int minOperations(vector<int>& nums, int x) {
            int n = nums.size();
            vector<int> pf(n);
            pf[0] = nums[0];
            for (int i = 1; i < n; i++) {
                pf[i] = pf[i - 1] + nums[i];
            }
            // Map to store prefix sums and their last index
            map<int, int> m;
            m[0] = -1; // to handle prefix sum equal to x from index 0
            for (int i = 0; i < n; i++) {
                m[pf[i]] = i;
            }
            int ans = INT_MAX;
            // Check if only prefix side is enough
            if (m.find(x) != m.end()) {
                ans = min(ans, m[x] + 1);
            }
            int sum = 0;
            for (int j = n - 1; j >= 0; j--) {
                sum += nums[j];
                int remain = x - sum;
                if (m.find(remain) != m.end() && m[remain] < j) {
                    ans = min(ans, m[remain] + 1 + (n - j));
                }
            }
            return (ans == INT_MAX ? -1 : ans);
        }
    };

Prefix_Sum_Only_Alternative:
  - If we consider only prefix sums (i.e., removing only from the left):
      - Just find smallest `i` such that `pf[i] == x`
      - This is already partially handled in the current code using:
          if (m.find(x) != m.end()) ans = m[x] + 1
  - It's optimal to combine both ends for flexibility, as in this full version.

12)Problem: Minimum Cost to Make Array Equal
Description:
  - You're given two arrays:
      - `nums[]`: the original values of elements
      - `cost[]`: cost per unit change for each corresponding `nums[i]`
  - You can change any `nums[i]` to any integer value.
  - The cost to change `nums[i]` to `x` is: `|nums[i] - x| * cost[i]`
  - Your task is to choose an integer `x` such that **total cost** to make **all nums[i] == x** is minimized.

Goal_Expression:
  - For a given index `i` with `nums[i] = x`, compute total cost to change all elements to `x`:
  - **Expression to minimize**:
    TotalCost(i) = Cost to make nums[0..i-1] = nums[i] (left)
                 + Cost to make nums[i+1..n-1] = nums[i] (right)
  - Let:
    - `v[i].first` = nums[i]
    - `v[i].second` = cost[i]

Prefix_Suffix_Sums_Usage:
  - Sort `(nums[i], cost[i])` by `nums[i]` to handle values in increasing order.
  
  - **Prefix Sums**:
    - `prefCost[i] = sum of cost[0..i]`
    - `prefWeighted[i] = sum of nums[j] * cost[j] for j = 0 to i`

  - **Suffix Info (via prefix)**:
    - Total cost: `prefCost[n-1]`
    - Total weighted: `prefWeighted[n-1]`

  - These help compute:
    - **LeftCost** (to increase elements before `i` to `nums[i]`):

      LeftCost = nums[i] * sum(cost[0..i-1]) - sum(nums[j]*cost[j] for j = 0 to i-1)
               = v[i].first * prefCost[i-1] - prefWeighted[i-1]

    - **RightCost** (to decrease elements after `i` to `nums[i]`):
      RightCost = sum(nums[j]*cost[j] for j = i+1 to n-1) - nums[i] * sum(cost[i+1..n-1])
                = (prefWeighted[n-1] - prefWeighted[i]) - v[i].first * (prefCost[n-1] - prefCost[i])

Time_Complexity: O(n log n)
  - Sorting: O(n log n)
  - Prefix sums + final loop: O(n)

Space_Complexity: O(n)
  - For prefix arrays

Code_Snippet:
  code: |
    class Solution {
    public:
        typedef long long ll;
        long long minCost(vector<int>& nums, vector<int>& cost) {
            int n = nums.size();
            vector<pair<ll, ll>> v(n); // store both nums[i] and cost[i] as ll
            for (int i = 0; i < n; i++) {
                v[i] = {static_cast<ll>(nums[i]), static_cast<ll>(cost[i])};
            }
            sort(v.begin(), v.end()); // sort by nums[i]
            // Build prefix sum of cost and weighted cost
            vector<ll> prefCost(n), prefWeighted(n);
            prefCost[0] = v[0].second;
            prefWeighted[0] = v[0].first * v[0].second;
            for (int i = 1; i < n; i++) {
                prefCost[i] = prefCost[i - 1] + v[i].second;
                prefWeighted[i] = prefWeighted[i - 1] + v[i].first * v[i].second;
            }
            ll total = prefCost[n - 1];
            ll ans = LLONG_MAX;
            for (int i = 0; i < n; i++) {
                ll leftCost = (i > 0) ? v[i].first * prefCost[i - 1] - prefWeighted[i - 1] : 0;
                ll rightCost = (i < n - 1) ? (prefWeighted[n - 1] - prefWeighted[i]) - v[i].first * (prefCost[n - 1] - prefCost[i]) : 0;
                ans = min(ans, leftCost + rightCost);
            }
            return ans;
        }
    };

Summary:
  - For each potential target `nums[i]`, use prefix and suffix sums to compute:
    - Cost to bring all previous elements up to `nums[i]`
    - Cost to bring all following elements down to `nums[i]`
  - Take the minimum across all possible targets.
  - This avoids computing absolute value differences directly and instead leverages efficient prefix summation.

13)Longest Consecutive Sequence:
  problem_statement: |
    Given an unsorted array of integers, find the length of the longest sequence 
    of consecutive integers. The sequence must consist of elements that appear 
    in the array and occur consecutively in numerical order. The order of the input 
    array does not matter.

    Example:
      Input: [100, 4, 200, 1, 3, 2]
      Output: 4
      Explanation: The longest consecutive sequence is [1, 2, 3, 4].

  code_snippet: |
    #include <bits/stdc++.h>
    using namespace std;
    typedef long long ll;
    int longestConsecutive(vector<ll>& nums) {
        set<ll> s;
        for (ll x : nums) s.insert(x); // Insert unique elements
        ll ans = 0;
        for (ll num : s) {
            // Consider only sequence starting points
            if (s.find(num - 1) == s.end()) {
                ll count = 0;
                ll temp = num;
                while (s.find(temp) != s.end()) {
                    count++;
                    temp++;
                }
                ans = max(ans, count);
            }
        }
        return ans;
    }

  approach:
    - Use a set to store all unique elements in the array.
    - Iterate through each unique number in the set.
    - For each number, check if it is the start of a new sequence.
    - A number is a start if there is no number before it (i.e., num - 1 is not in the set).
    - From each valid start point, count the length of the consecutive sequence by checking 
      for the presence of num + 1, num + 2, etc.
    - Track the maximum sequence length encountered during the process.

  key_logics:
    only_unique_values:
      description: |
        The sequence should be built only from unique elements. Duplicates don't extend the 
        length of a consecutive sequence. Hence, we use a `set` which stores only distinct values.
        This avoids recomputing the same sequences multiple times and simplifies lookups.
      structure_used: set
      benefit: |
        - Removes duplicates automatically
        - Allows O(log n) or O(1) average-time lookups
        - Guarantees each number is checked only once

    check_previous_number_absence:
      logic: |
        A number should be considered as the **start** of a sequence only if the number just 
        before it (num - 1) is NOT present in the set.
      reason: |
        - Prevents redundant counting of numbers already part of another sequence.
        - Ensures that each sequence is counted **only once**, starting from its smallest number.
        - Example: In [1, 2, 3, 4], only 1 qualifies as a start. Checking 2, 3, 4 as starts 
          would lead to repeated computation.

    count_consecutive_forward:
      logic: |
        Once a start of sequence is found (i.e., num - 1 is not in set), we incrementally check
        for num + 1, num + 2, ..., as long as those elements are present in the set.
      implementation: |
        - Use a loop that increases `temp` and checks `set.find(temp) != set.end()`
        - Maintain a `count` to record how many consecutive numbers are found
        - Update the `ans` (maximum length) accordingly

  complexity_analysis:
    time_complexity: |
      O(n log n) — where n is the size of input array
      - Inserting all elements into a `set` takes O(n log n)
      - Each number is processed at most once, and each lookup is O(log n)

    space_complexity: |
      O(n) — for storing all unique elements in the set

  optimization_note:
    better_data_structure: unordered_set
    benefit: |
      - Using `unordered_set` can reduce average lookup time to O(1)
      - That brings overall time complexity down to O(n) on average

  example_dry_run:
    input: [100, 4, 200, 1, 3, 2]
    set_built: [1, 2, 3, 4, 100, 200]
    sequence_detections:
      - num: 1
        is_start: true
        sequence: [1, 2, 3, 4]
        length: 4
      - num: 2
        is_start: false
        skipped: true
      - num: 3
        is_start: false
        skipped: true
      - num: 4
        is_start: false
        skipped: true
      - num: 100
        is_start: true
        sequence: [100]
        length: 1
      - num: 200
        is_start: true
        sequence: [200]
        length: 1
    output: 4

  conclusion: |
    The key to solving the longest consecutive sequence problem efficiently is to:
    - Avoid redundant work by identifying true sequence starting points
    - Use a hash set or ordered set to allow fast presence checking
    - Track the length of only those sequences that haven’t been processed before
    This leads to a clean, efficient solution with optimal time and space usage.

14)Problem Name: Maximum Distance in Arrays
Problem Statement:
  - You are given a list of arrays, where each array is sorted in increasing order.
  - You need to pick one element from one array and another element from a different array to maximize the absolute difference between these two elements.
  - Return the maximum such absolute difference.

Observation:
  - Since arrays are sorted:
      - The minimum value of any array will be at index 0 (first element).
      - The maximum value of any array will be at the last index (last element).

Goal:
  - Maximize |a - b| such that:
      - a is from one array, b is from another.
      - So the pair (min, max) should be from different arrays.

Brute Force Approach:
  - Compare each pair of arrays (O(n²)), calculate all possible differences using min of one and max of another.
  - This will TLE for large input.

Optimized Greedy Approach:
  - Since we only care about maximum and minimum across different arrays,
    we can:
      - Track the **overall smallest min** and **largest max** values,
      - But ensure they come from **different arrays**.
  - If max and min come from the same array, we look at second-best max or min.

Steps:
  1. For each array, store:
      - `mini[i] = {first element, index}`
      - `maxi[i] = {last element, index}`
  2. Sort both `mini` and `maxi` lists.
  3. Try to pick min and max from different indices:
      - If they are from different arrays → done.
      - If same → check next best option (second min or second max).

Greedy Logic:
  - Greedy works here because we only need the best global min and max — we only go to second-best if the best ones are from the same array.

Time Complexity:
  - O(n log n) for sorting min and max arrays.

Space Complexity:
  - O(n) for storing min and max pairs.

Code Snippet (Greedy with Sorting):
  ```cpp
  int maxDistance(vector<vector<int>>& arrays) {
      int n = arrays.size();
      vector<pair<int, int>> mini(n), maxi(n);
      for (int i = 0; i < n; i++) {
          mini[i] = {arrays[i][0], i};             // store min with array index
          maxi[i] = {arrays[i].back(), i};         // store max with array index
      }
      sort(mini.begin(), mini.end());             // sort by value
      sort(maxi.begin(), maxi.end());             // sort by value
      int s = 0, e = n - 1;
      while (s <= e) {
          if (mini[s].second != maxi[e].second) {
              return maxi[e].first - mini[s].first;
          } else {
              // Try skipping one of them if both belong to same array
              int a = maxi[e].first - mini[s + 1].first;
              int b = maxi[e - 1].first - mini[s].first;
              if (a <= b) e--;
              else s++;
          }
      }
      return 0;
  }

15)Problem: Minimum Increments to Make Array Elements Unique
Goal:
  We are given an array `nums` of integers.
  The objective is to make **all elements unique** using the **minimum number of operations**.
  In one operation, you can **increment any element by 1**.
  Return the total number of increments (i.e., operations) required to achieve uniqueness.

Code Snippet:
  ```cpp
  #include <bits/stdc++.h>
  using namespace std;
  int minIncrementForUnique(vector<int>& nums) {
      int n = nums.size();
      sort(nums.begin(), nums.end());     // Step 1: Sort the array
      int ans = 0;                         // Total number of increments
      int last = nums[0];                 // Tracks the last unique value used
      for (int i = 1; i < n; i++) {
          if (nums[i] <= last) {          // Current value is not unique
              ans += (last + 1 - nums[i]);  // Add the needed increment to 'ans'
              last = last + 1;              // Update 'last' to new unique value
          } else {
              last = nums[i];              // Already unique; update 'last'
          }
      }
      return ans;
  }

16)Problem: 4Sum II
Description: >
  Given four integer arrays nums1, nums2, nums3, and nums4 all of length n,
  return the number of tuples (i, j, k, l) such that:
  
      nums1[i] + nums2[j] + nums3[k] + nums4[l] == 0

Intuition: >
  Instead of checking all 4-tuples using 4 nested loops (O(n^4)), we can optimize
  by breaking the problem into two parts:

  Let A = nums1[i] + nums2[j] and B = nums3[k] + nums4[l]
  We want A + B == 0 → i.e., A == -B

  We can compute all possible B values and store their frequencies in a hash map.
  Then, for every A value, we just check how many times -A occurred in that map.
  This reduces the problem to O(n^2) time.

Approach:
  Step 1:
    - Initialize an empty map<int, int> called freq_map.
    - Iterate over all pairs (i, j) from nums3 and nums4.
    - For each pair, compute sum = nums3[i] + nums4[j] and increment freq_map[sum].
    
  Step 2:
    - Initialize ans = 0
    - Iterate over all pairs (i, j) from nums1 and nums2.
    - For each pair, compute sum = nums1[i] + nums2[j]
    - Look for -sum in freq_map:
        - If present, add freq_map[-sum] to ans.
  
  Step 3:
    - Return ans as the final count of valid quadruplets.

Time Complexity: "O(n^2)"
  Justification: >
    - Two nested loops to compute all pairwise sums of nums3 and nums4 → O(n^2)
    - Two nested loops to compute sums of nums1 and nums2 and lookup in map → O(n^2)
    - Hash map lookups are O(1) on average

Space Complexity: "O(n^2)"
  Justification: >
    - In the worst case, there can be n^2 unique sums from nums3 + nums4
    - These are stored in the map, hence O(n^2) space

Edge Cases:
  - If arrays are empty → returns 0 (n=0, loops don't run)
  - If all arrays contain only zeros → outputs large count (all combinations valid)
  - Handles duplicates correctly due to frequency counting

Code (C++):
  snippet: |
    class Solution {
    public:
        int fourSumCount(vector<int>& nums1, vector<int>& nums2, vector<int>& nums3, vector<int>& nums4) {
            int n = nums1.size();
            map<int, int> m;
            // Step 1: Build hashmap of all sums of elements from nums3 and nums4
            for (int i = 0; i < n; i++) {
                for (int j = 0; j < n; j++) {
                    int temp = nums3[i] + nums4[j];
                    m[temp]++;
                }
            }
            int ans = 0;
            // Step 2: For each pair from nums1 and nums2, check for -sum in map
            for (int i = 0; i < n; i++) {
                for (int j = 0; j < n; j++) {
                    int temp = nums1[i] + nums2[j];
                    if (m.find(-temp) != m.end()) {
                        ans += m[-temp];
                    }
                }
            }
            // Step 3: Return total count
            return ans;
        }
    };

Optimizations:
  - Can use unordered_map for better average time complexity.
  - Can swap roles of first and second half depending on data size (symmetry).

Alternative Approaches:
  - Brute force (4 loops): O(n^4) — not acceptable for n > 100.
  - Meet-in-the-middle (used here): Best tradeoff for O(n^2) performance.

17)Problem: Median Finder – Detailed `addNum()` Explanation with Multisets
Context:
  We want to maintain a running list of numbers and find the median efficiently at any point.
  This requires:
    - Fast insertion
    - Efficient median access (O(1) or O(log n))

  We use two **multisets** to simulate min-heap and max-heap:
    - `left` → stores the smaller half of elements (like max-heap)
    - `right` → stores the larger half of elements (like min-heap)
  
  Invariant to Maintain:
    - All elements in `left` ≤ all elements in `right`
    - Size(left) >= Size(right)
    - Difference in sizes is at most 1

Explanation of `addNum()`:
  Goal:
    Insert the number while maintaining the heap invariant and order relation.

  Logic:
    1. Insert into `left` if it's empty or its size is less than or equal to `right`.
       Why? So we always try to balance towards `left` unless it's too big.

    2. Otherwise, insert into `right`.

    3. Rebalance if required:
       If the largest of `left` is greater than the smallest of `right`,
       swap their elements to maintain ordering constraint.

Main Code Snippet:
  ```cpp
  multiset<int> left, right; // Declare globally or inside class
  void addNum(int num) {
      if (left.empty() || left.size() <= right.size()) {
          left.insert(num);
      } else {
          right.insert(num);
      }
      // Rebalance step: fix ordering if necessary
      if (!left.empty() && !right.empty() && *left.rbegin() > *right.begin()) {
          int maxLeft = *left.rbegin();   // Largest in left
          int minRight = *right.begin();  // Smallest in right
          left.erase(left.find(maxLeft));
          right.erase(right.find(minRight));
          left.insert(minRight);
          right.insert(maxLeft);
      }
  }

18)Problem: Minimum Operations to Reduce Array to Zero with At Most x Reductions per Operation
Description: |
  You are given an array `a` of size `n` consisting of non-negative integers.
  In one operation, you can choose at most `x` distinct elements of the array
  and decrease each of the chosen elements by exactly 1.

  The goal is to determine the minimum number of operations needed to reduce
  all elements of the array to zero.

Constraints:
  - Each element of the array `a[i]` can only be reduced by 1 in a single operation.
  - You may only select at most `x` elements in any one operation.
  - No element can be reduced by more than 1 per operation.

Key Observations:
  - Every element `a[i]` needs at least `a[i]` operations, since it can be reduced by at most 1 per operation.
  - You can only reduce up to `x` elements per operation.
  - So, across the entire array, you must do at least `ceil(sum(a) / x)` operations to reduce the total units of value.
  - However, an element with large value (e.g., 100) still requires 100 operations on its own, no matter how many you can reduce at a time.

Formula:
  min_operations = max(max(a), ceil(sum(a) / x))

Explanation of Terms:
  - max(a): The largest value in the array; represents the minimum number of operations required to reduce the most stubborn (largest) element.
  - sum(a): The total value across all elements; represents the total work to be done.
  - x: Number of elements you can reduce in a single operation.
  - ceil(sum(a) / x): Minimum number of operations required to reduce the total work, assuming perfect parallelism.

Example 1:
  Input:
    a: [4, 3, 2]
    x: 2
  sum: 9
  max(a): 4
  ceil(9 / 2): 5
  Output: 5

Example 2:
  Input:
    a: [4, 4, 4]
    x: 2
  sum: 12
  max(a): 4
  ceil(12 / 2): 6
  Output: 6

Final Result:
  The correct and optimized formula to compute the minimum number of operations is:
  min_operations = max(max(a), ceil(sum(a) / x))

19)Problem: Point Inside or On Triangle Using Area Method
Description: |
  Given a triangle defined by 3 points A(x1, y1), B(x2, y2), and C(x3, y3),
  and a list of query points P(px, py), determine whether each point lies:
    - strictly inside the triangle
    - on the edge of the triangle
    - outside the triangle

Approach: |
  We use the determinant-based area formula (also known as the shoelace formula) to determine the triangle's area.
  For a point P(px, py), compute the area of triangle ABC, and the areas of sub-triangles:
    - Area1 = Area of triangle PBC
    - Area2 = Area of triangle APC
    - Area3 = Area of triangle ABP

  If the sum of Area1 + Area2 + Area3 is approximately equal to Area(ABC), then point P lies inside or on the triangle.

  Then:
    - If any of the sub-areas is 0, point lies on the corresponding edge.
    - If all sub-areas are positive, point lies strictly inside.
    - If sum of sub-areas is not approximately equal to Area(ABC), point lies outside.

Code_Snippet_C++: |
  double area(int x1, int y1, int x2, int y2, int x3, int y3) {
      return 0.5 * abs(
          x1 * (y2 - y3) +
          x2 * (y3 - y1) +
          x3 * (y1 - y2)
      );
  }
  // Check if point (px, py) lies inside or on triangle ABC
  string checkPoint(int x1, int y1, int x2, int y2, int x3, int y3, int px, int py) {
      double A = area(x1, y1, x2, y2, x3, y3);
      double A1 = area(px, py, x2, y2, x3, y3);
      double A2 = area(x1, y1, px, py, x3, y3);
      double A3 = area(x1, y1, x2, y2, px, py);
      double sum = A1 + A2 + A3;
      if (abs(sum - A) > 1e-9) return "outside";  // handle floating point precision
      if (A1 == 0 || A2 == 0 || A3 == 0) return "on";  // lies on an edge
      return "inside";
  }

Why_Not_Using_Exact_Equality: |
  - Due to floating-point precision limitations, directly checking:
      if (A1 + A2 + A3 == A)
    may fail even if mathematically correct.
  - This is because operations like multiplication and division with doubles
    can introduce tiny rounding errors.
  - So, we use:
      if (abs(sum - A) > 1e-9)
    to check if the difference is negligibly small (within an epsilon tolerance).

Why_Order_Does_Not_Matter: |
  - The determinant-based area formula gives the same unsigned area regardless of point order.
  - That’s because:
      Area(A, B, C) == Area(C, A, B) == Area(B, C, A) after taking absolute value.
  - We use abs(...) in the area function, which ensures result is always non-negative.
  - So, you can pass the triangle points in any order when computing area — the logic will still work.

Example_Usage:
  Triangle: [(0, 0), (5, 0), (0, 5)]
  Query_Points:
    - (1, 1): "inside"
    - (0, 0): "on"
    - (6, 6): "outside"
    - (2, 3): "inside"

Time_Complexity: |
  - For each point check: O(1)
  - Total for k query points: O(k)

Final_Result: |
  For each query point:
    - Compute three sub-areas: (PBC), (PAC), (PAB)
    - If their sum ≈ triangle's area: point is inside or on
    - If any sub-area = 0: point is on edge
    - Else: point is outside

  This method is efficient, precise, and works for all triangle orientations.

20)Problem: Find the Winning Player
Intuition:
  - This problem simulates a game where players face off in order.
  - The first two players are compared: the one with higher skill wins.
  - The winner stays at the front, the loser goes to the back.
  - We track how many consecutive wins each player has.
  - The first player to win `k` times is the winner.
  - If no player reaches `k` wins early, the player with the maximum skill value will eventually dominate.

Approach:
  step_by_step:
    - Create a map `m` to store the mapping from skill value to its index (since players move around).
    - If `k >= n`, the game ends when the strongest player wins: return the index of max skill.
    - Initialize a deque `dq` with all the skills.
    - Maintain a vector `count[n]` to track consecutive wins for each player (using original index).
    - Simulate the game:
        - Pop first two elements from the deque.
        - Compare their skills.
        - Winner goes to the front, loser to the back.
        - Increment the win count for the winner.
        - If any player’s win count reaches `k`, return their index.
    - If no one reaches `k`, strongest player wins in ≤ n + k rounds.

Code:
  language: cpp
  snippet: |
    class Solution {
    public:
        int findWinningPlayer(vector<int>& skills, int k) {
            int n = skills.size();
            vector<int> count(n, 0);
            map<int, int> m;
            for (int i = 0; i < n; i++) m[skills[i]] = i;
            if (k >= n)return max_element(skills.begin(), skills.end()) - skills.begin();
            deque<int> dq;
            for (int i = 0; i < n; i++) dq.push_back(skills[i]);
            while (!dq.empty()) {
                int first = dq.front(); dq.pop_front();
                int second = dq.front(); dq.pop_front();
                if (first > second) {
                    dq.push_front(first);
                    dq.push_back(second);
                    count[m[first]]++;
                    if (count[m[first]] == k) return m[first];
                } else {
                    dq.push_front(second);
                    dq.push_back(first);
                    count[m[second]]++;
                    if (count[m[second]] == k) return m[second];
                }
            }
            return 0;
        }
    };

Deque_Usage:
  reason: >
    We use `deque` because it allows O(1) insertion and deletion from both ends.
    - push_front(): to place the winner back at the front.
    - push_back(): to place the loser at the back.
    - pop_front(): to remove two players for each round.
    This flexibility is not available in a standard queue.

Edge_Cases:
  - k >= n: return index of max skill player.
  - All players have same skill: first one wins.

Return:
  - Index of the player who first wins `k` matches.
  - Based on original position in the array.

Tags: [Deque, Simulation, Greedy, Game Logic, Queue]

21)Problem: Gas Station - Find the Starting Point to Complete the Circuit
Description:
  You are given two integer arrays `gas` and `cost` of length `n`, where:
    - `gas[i]` is the amount of gas available at station i.
    - `cost[i]` is the cost of gas to travel from station i to station (i + 1) % n.
  
  Return the index of the gas station to start from so that you can travel around the circuit once in the clockwise direction without running out of gas. If no such starting point exists, return -1.

Approach: Greedy (Optimized O(n) solution)
Greedy Strategy:
  - Compute net gas at each station: `temp[i] = gas[i] - cost[i]`.
  - Maintain two variables:
      - `total`: Tracks the total net gas. If `total < 0`, return -1 immediately — the journey is impossible.
      - `current`: Tracks the running fuel level. If `current < 0`, it means we cannot reach the next station.
        Hence, we move the starting index to `i + 1`, as all previous points are invalid.
  - Final answer is the first such `start` index where we never go below 0, provided the total gas is sufficient.

Why current < 0 implies reset:
  - If `current` (running balance) drops below zero at station `i`, then **starting from the previous start up to i has failed**.
  - Any station before or between would not be able to reach station `i+1` either.
  - Therefore, we greedily skip the entire failed segment and attempt starting from `i + 1`.

Time and Space Complexity:
  Time: O(n) — single pass through all stations
  Space: O(n) due to temp[] array (can be optimized to O(1) if temp is computed inline)

Dry Run on Input:
  gas:  [1, 2, 3, 4, 5]
  cost: [3, 4, 5, 1, 2]
  temp: [-2, -2, -2, +3, +3]
  total = -2 -2 -2 +3 +3 = 0 → gas is just enough, so it's possible

  Iteration:
    i = 0 → current = -2 → reset start to 1
    i = 1 → current = -2 → reset start to 2
    i = 2 → current = -2 → reset start to 3
    i = 3 → current = +3
    i = 4 → current = +6

  Final result: total = 0 ≥ 0 → return ans = 3

Code Snippet (with main):
  #include <iostream>
  #include <vector>
  using namespace std;
  // Function to find the starting gas station index
  int canCompleteCircuit(vector<int>& gas, vector<int>& cost) {
      int n = gas.size();
      vector<int> temp(n);
      // Step 1: Compute net gas at each station
      for (int i = 0; i < n; i++) {
          temp[i] = gas[i] - cost[i];
      }
      int total = 0;     // Total gas available across all stations
      int current = 0;   // Running fuel balance
      int ans = 0;       // Candidate starting index
      // Step 2: Traverse all stations
      for (int i = 0; i < n; i++) {
          total += temp[i];
          current += temp[i];
          // If we run out of fuel, we must start from the next station
          if (current < 0) {
              current = 0;
              ans = i + 1;  // try next station as starting point
          }
      }
      // Step 3: If total gas is enough, return start index
      return (total >= 0) ? ans : -1;
  }

22)Problem: Stock Span Problem using Previous Greater Element (PGE)
Explanation:
  Goal:
    - For each day `i`, compute the **maximum number of consecutive days before (and including) day `i`** 
      where the stock price was **less than or equal to** the price on day `i`.

  Core Idea:
    - This is a classic **Previous Greater Element (PGE)** based problem.
    - We compute the index of the **last day before i** that had a **higher price than arr[i]**.
    - The span is then simply the width between that index and current index: `i - left[i]`.

  Step-by-step Implementation:
    1. Initialize:
        - `n`: Length of input array `arr`.
        - `left[i]`: For each `i`, stores index of the **Previous Greater Element (PGE)**.
          - If no such element exists, `left[i] = -1`.
        - `ans[i]`: Final result where each value is the span for that day.
        - `stack<int> st`: Used to find the PGE efficiently in O(n) time.
    2. Loop through the array from left to right:
        - While stack is not empty and the price at the top of stack is less than or equal to current:
            - Pop the stack (it can't be PGE anymore).
        - If the stack is not empty after popping:
            - The top of stack is the PGE index → `left[i] = st.top()`.
        - If the stack is empty:
            - There is no previous greater → `left[i] = -1` (already default).
        - Push current index `i` onto the stack.
    3. After the loop:
        - For each `i`, compute span: `ans[i] = i - left[i]`.
    4. Why it works:
        - The span is the number of days from the last day with a **greater price** (exclusive) up to day `i` (inclusive).
        - Using monotonic stack (decreasing) ensures we always maintain nearest previous greater efficiently.

Code_Snippet:
  - |
    vector<int> calculateSpan(vector<int>& arr) {
        int n = arr.size();
        vector<int> left(n, -1), ans(n, 0); // stores index of previous greater
        stack<int> st;
        for (int i = 0; i < n; i++) {
            while (!st.empty() && arr[st.top()] <= arr[i]) {
                st.pop(); // pop while stack top is smaller or equal
            }
            if (!st.empty()) {
                left[i] = st.top(); // PGE index
            }
            st.push(i); // push current index
        }
        for (int i = 0; i < n; i++) {
            ans[i] = i - left[i]; // width from last greater to i
        }
        return ans;
    }

23)Problem: Max Number of K-Sum Pairs
Description: |
  Given an array of integers `nums` and an integer `k`, return the maximum number of unique pairs `(i, j)` such that:
  - `i < j`
  - `nums[i] + nums[j] == k`
  - Each element in `nums` can be used **at most once**
  -Yes we want to count number of distinct pairs non overalpping pairs which sum to k 

Explanation:
  Approach:
    - Use a hash map (`map<int, int> m`) to count the frequency of each number in the array.
    - Iterate over all unique numbers in the map:
        - For each number `val`, calculate its complement `k - val` (let's call it `temp`)
        - If `temp` also exists in the map, we can pair `val` and `temp`
        - Add the minimum of the two frequencies to `ans`
    - Divide the final answer by 2 because:
        - Each valid pair is counted **twice** (once for `val`, once for `k - val`)
        - We want the total number of **distinct pairs**, so we halve the count

  Why It Works:
    - Handles duplicates using frequency count
    - Works for both:
        - Different elements forming a pair (e.g. 1 + 5 = 6)
        - Same elements forming a pair (e.g. 3 + 3 = 6), but only if `freq[3] >= 2`
    - Works for **odd frequencies** too: `min(freq[a], freq[b])` ensures only complete pairs are counted

  Example:
    Input: nums = [1,2,3,4], k = 5
    Pairs: (1,4), (2,3) → Answer = 2
    Input: nums = [3,1,3,4,3], k = 6
    - freq[3] = 3
    - freq[3] & 3 can make at most 1 full pair → Answer = 1

Code:
  main_code: |
#include <bits/stdc++.h>
using namespace std;
int maxOperations(vector<int>& nums, int k) {
    int n = nums.size();
    // Frequency map
    map<int, int> m;
    for(int i = 0; i < n; i++) m[nums[i]]++;

    int ans = 0;
    for(auto it = m.begin(); it != m.end(); it++) {
        int val = it->first;
        int temp = k - val;
        if(m.find(temp) != m.end()) {
            auto tempit = m.find(temp);
            ans += min(it->second, tempit->second);
        }
    }
    return ans / 2; // Each pair counted twice
}

24)Max Points on a Line:
  problem_statement: |
    Given a list of 2D points, return the maximum number of points that lie on the same straight line.
    Each line can be uniquely defined by a slope. Points that share the same slope with a reference point lie on the same line.

  key_idea: |
    - Fix one point as the reference.
    - Compute the slope between this reference point and every other point.
    - Store the count of points that share the same slope in a map.
    - Keep track of duplicate points (same as the reference).
    - The maximum number of points on a line through the reference is:
      max count of same slope + number of duplicates.
    - Repeat this for every point as reference and track the global maximum.

  steps:
    - For every point in the list:
      - Treat it as the reference point.
      - Initialize a hashmap (map<double, int>) to store slope counts.
      - Initialize a counter for duplicate points.
      - For each other point:
        - If it’s a duplicate (same x and y), increment duplicate count.
        - If it forms a vertical line (same x), use a sentinel slope (e.g. 1e9).
        - Otherwise, compute slope using (double)(y2 - y1) / (x2 - x1) to avoid integer division.
      - After iterating over all points:
        - Find the maximum number of points that lie on a single line from this reference point.
        - Update the global answer using: maxLineCount + duplicates.

  edge_cases_handled:
    - vertical_lines: Represented with a special slope (e.g., 1e9) to avoid division by zero.
    - duplicate_points: Counted separately and added to every line count passing through the reference.
    - integer_division: Avoided by explicitly casting to double during slope calculation.

  dry_run:
    input: [[1,1], [2,2], [3,3], [1,1]]
    reference_point: [1,1]
    slopes_computed:
      - slope with [2,2]: 1
      - slope with [3,3]: 1
      - [1,1]: duplicate
    result_for_this_reference: 2 points on slope 1 + 1 duplicate = 3 points on same line
    final_output: 3

  code: |
    class Solution {
    public:
        int maxPoints(vector<vector<int>>& points) {
            int ans = 1; // At least one point exists
            for (auto& point : points) {
                int px = point[0], py = point[1];
                map<double, int> mp;
                int duplicates = 0;
                for (auto& subpoint : points) {
                    int x = subpoint[0], y = subpoint[1];
                    if (x == px && y == py) {
                        // Count duplicates
                        duplicates++;
                    } else if (x == px) {
                        // Vertical line — slope is undefined, use sentinel value
                        mp[1e9]++;
                    } else {
                        // Normal slope calculation (cast to avoid integer division)
                        double m = (double)(y - py) / (x - px);
                        mp[m]++;
                    }
                }
                int maxLine = 0;
                for (auto& [slope, count] : mp) {
                    maxLine = max(maxLine, count);
                }
                // Add duplicate count (including reference point itself)
                ans = max(ans, maxLine + duplicates);
            }
            return ans;
        }
    };

  complexity:
    time: O(n^2)
    space: O(n)
    explanation: |
      - For each of the n points, we compare it with every other point → O(n^2).
      - For each reference point, we store at most (n-1) slopes in the map → O(n) space per outer iteration.
      - Total space is still O(n) because we reuse the map every time.

  notes:
    - You could further optimize precision by storing reduced fractions using pair<int, int> as slope instead of double.
    - This avoids floating-point errors in extreme cases or precision-sensitive input.

25)Problem: Find K Pairs with Smallest Sums
Description: >
  Given two sorted arrays nums1 and nums2 of size n and m respectively, 
  return the k pairs (u,v) such that u is from nums1 and v is from nums2 
  and the sum u + v is the smallest possible among all possible combinations.

Approach:
  Intuition: >
    We need the k pairs with the smallest sums. The brute-force approach would be
    to generate all n*m pairs, compute their sums, sort them, and return the first k.
    But this takes O(n*m*log(n*m)) time, which is slow for large arrays.

    We can optimize this using a **min-heap (priority queue)**. Since the arrays are sorted,
    the smallest pair is (nums1[0], nums2[0]), and potential next smaller sums can come from
    moving in either direction: (nums1[1], nums2[0]) or (nums1[0], nums2[1]).

    So we push the initial pair into a min-heap, and from each popped pair,
    we explore its right and down neighbors (i+1,j) and (i,j+1) to push into the heap.

  Why Priority Queue (Min-Heap) is used: >
    We want the smallest sums first. So we use a min-heap where the top of the heap always
    gives us the next minimum pair sum. Each element in the heap is a vector of the form:
    [sum, i, j] -> where nums1[i] + nums2[j] = sum.

  Why we track visited pairs using a set: >
    When we explore (i+1, j) and (i, j+1), we may encounter the same pair multiple times
    due to different paths reaching it. To avoid duplicate work and pushing the same pair 
    into the heap more than once, we use a set to mark (i,j) pairs we've already visited.

  Why we add both (i+1, j) and (i, j+1): >
    This ensures we're not greedy in only one direction. The smallest future pairs could
    lie in both directions, so we must explore both paths. This is similar to how BFS
    explores neighbors — we allow the heap to order the exploration based on actual sums.

Code:
  language: cpp
  main_code: |
    #include <bits/stdc++.h>
    using namespace std;
    vector<vector<int>> kSmallestPairs(vector<int>& nums1, vector<int>& nums2, int k) {
        int n = nums1.size(), m = nums2.size();
        vector<vector<int>> ans;
        // Min-heap: stores [sum, i, j]
        priority_queue<vector<int>, vector<vector<int>>, greater<vector<int>>> pq;
        // Set to track visited (i, j) index pairs
        set<pair<int, int>> visited;
        // Start with the smallest pair
        pq.push({nums1[0] + nums2[0], 0, 0});
        visited.insert({0, 0});
        while (!pq.empty() && ans.size() < k) {
            auto top = pq.top(); pq.pop();
            int i = top[1], j = top[2];
            ans.push_back({nums1[i], nums2[j]});
            // Try moving to (i+1, j) if not visited
            if (i + 1 < n && visited.find({i + 1, j}) == visited.end()) {
                pq.push({nums1[i + 1] + nums2[j], i + 1, j});
                visited.insert({i + 1, j});
            }
            // Try moving to (i, j+1) if not visited
            if (j + 1 < m && visited.find({i, j + 1}) == visited.end()) {
                pq.push({nums1[i] + nums2[j + 1], i, j + 1});
                visited.insert({i, j + 1});
            }
        }
        return ans;
    }

Time Complexity: >
  - In the worst case, we insert up to min(k, n*m) pairs into the heap.
  - Each insertion and deletion from the heap takes log(heap size) time.
  - So total time is O(k log k).

Space Complexity: >
  - Heap size up to O(k)
  - Set to track visited pairs: also up to O(k)
  - Output array of size k

26)Count_LCT_Subsequences:
  problem: Count the number of subsequences of the form "LCT" in a given string
  description: |
    Given a string consisting of characters 'L', 'C', and 'T', we are to count 
    how many subsequences of the form "LCT" exist. A subsequence means that the 
    characters appear in order but not necessarily contiguously.
    
    For example, in the string "LLCCCTT", there are 12 different "LCT" subsequences.

  approach: |
    1. Use prefix sums to track the number of 'L's and "LC" subsequences up to each index.
    2. For each character:
        - If it's 'L', increment prefixL[i]
        - If it's 'C', use the number of 'L's before it to add to prefixLC[i]
        - If it's 'T', add the number of "LC" subsequences before it to the final count.
    3. This approach ensures that we respect the order of characters in the subsequence.

  code: |
    #include <bits/stdc++.h>
    using namespace std;
    typedef long long ll;
    int main() {
        // Given a string, count number of subsequences of the form "LCT"
        string s;
        cin >> s;
        ll n = s.size();
        // Step 1: Count number of 'L's up to each index
        vector<ll> pfl(n); // pfl[i] = prefix count of 'L' till index i
        pfl[0] = (s[0] == 'L') ? 1 : 0;
        for (ll i = 1; i < n; i++) {
            pfl[i] = pfl[i - 1] + ((s[i] == 'L') ? 1 : 0);
        }
        // Step 2: Count number of "LC" subsequences up to each index
        vector<ll> pflc(n); // pflc[i] = prefix count of "LC" till index i
        pflc[0] = 0;
        for (ll i = 1; i < n; i++) {
            pflc[i] = pflc[i - 1];
            if (s[i] == 'C') {
                pflc[i] += pfl[i]; // every 'L' before this 'C' makes an "LC"
            }
        }
        // Step 3: For every 'T', add the number of "LC" before it
        ll ans = 0;
        for (ll i = n - 1; i >= 0; i--) {
            if (s[i] == 'T') {
                ans += pflc[i]; // each "LC" before this 'T' gives an "LCT"
            }
        }
        cout << ans << endl;
        return 0;
    }

  example:
    input: "LLCCCTT"
    output: 12
    explanation: |
      There are multiple ways to choose 'L', 'C', and 'T' in increasing index order
      such that they form subsequences "LCT". Total such valid combinations = 12.

27)problem_name: Maximum Number of "LCT" Subsequences After One Insertion
intuition: |
  We are given a string consisting of characters 'L', 'C', and 'T'. Our goal is to count the number of "LCT" subsequences and also check the best position to insert one extra character ('L', 'C', or 'T') to maximize the count of "LCT" subsequences.
  - A valid "LCT" subsequence means choosing indices `i < j < k` such that `s[i]='L'`, `s[j]='C'`, `s[k]='T'`.
  - If we insert an additional 'L', 'C', or 'T' anywhere in the string (including before the first character and after the last), how much can the count increase?
  - The strategy is to use prefix and suffix arrays to precompute helpful information, and then evaluate all `n + 1` insertion points.

approach: |
  1. **Prefix Count of 'L'** (`pfl[i]`): number of 'L's from index 0 to i.
  2. **Suffix Count of 'T'** (`sft[i]`): number of 'T's from index i to n-1.
  3. **Prefix Count of "LC" pairs** (`pflc[i]`): for each 'C' at index `i`, count how many 'L's came before it.
  4. **Suffix Count of "CT" pairs** (`sfct[i]`): for each 'C' at index `i`, count how many 'T's come after it.

  5. **Count Existing "LCT" Subsequences**:
     - Iterate from right to left. For every 'T' at index `i`, add the number of "LC" pairs before `i` to `temp`.

  6. **Try Inserting 'L', 'C', or 'T' at Every Position (0 to n)**:
     - `Insert L at position i`: This 'L' can combine with all "CT" subsequences starting at index `i` or after. So we add `sfct[i]` to `temp`.
     - `Insert C at position i`: This 'C' can pair with all 'L's before `i` and 'T's after `i`. So we add `pfl[i-1] * sft[i]` to `temp`.
     - `Insert T at position i`: This 'T' can pair with all "LC" subsequences before index `i`. So we add `pflc[i-1]` to `temp`.

  7. **Take the Maximum** of all these values to get the answer.

code_steps:
  - Step 1: Compute `pfl` → prefix sum of 'L'
  - Step 2: Compute `sft` → suffix sum of 'T'
  - Step 3: Compute `pflc` → for every 'C', count how many 'L's came before it
  - Step 4: Compute `sfct` → for every 'C', count how many 'T's came after it
  - Step 5: Count all original "LCT" subsequences using `T` positions and `pflc` values before them
  - Step 6: Try inserting 'L', 'C', or 'T' at every index from 0 to n:
      - `insert_L = temp + sfct[i]` if `i < n`
      - `insert_C = temp + pfl[i - 1] * sft[i]` if `i > 0` and `i < n`
      - `insert_T = temp + pflc[i - 1]` if `i > 0`
  - Step 7: Take max of all these possibilities as final answer.

time_complexity: |
  - O(n) for building each prefix/suffix array (`pfl`, `sft`, `pflc`, `sfct`)
  - O(n) for counting original "LCT"
  - O(n) for trying insertions at `n + 1` positions
  - Total: **O(n)**

space_complexity: |
  - O(n) for each of the 4 prefix/suffix arrays → `pfl`, `sft`, `pflc`, `sfct`
  - Total: **O(n)**

key_observation_on_insertions: |
  - The insertion is tried at `n + 1` positions (from index 0 to n, inclusive).
  - At each position, inserting a single character is treated as "creating new potential LCT subsequences":
    - New L adds value via future CTs (`sfct[i]`)
    - New C adds value via past Ls and future Ts (`pfl[i-1] * sft[i]`)
    - New T adds value via past LCs (`pflc[i-1]`)
  - We always add `temp` because we are considering improvement *on top of* the original subsequences.

final_formula_examples: |
  - insert_L = temp + sfct[i]
  - insert_C = temp + pfl[i-1] * sft[i]
  - insert_T = temp + pflc[i-1]

28)BinaryExponentiationModular:
  title: "Binary Exponentiation for a^b mod M"
  description: |
    Binary exponentiation (also called fast exponentiation) is an efficient
    way to compute `a^b mod M` in O(log b) time. Instead of multiplying
    `a` repeatedly `b` times, we use the binary representation of `b` to
    square the base and reduce the exponent quickly.

    The main idea:
    - Represent `b` in binary (e.g., b = 13 → 1101 in binary).
    - For each bit from LSB to MSB:
      - If the bit is `1`, multiply the result by the current base (mod M).
      - Square the base (mod M) for the next bit.
      - Shift `b` right by 1 (equivalent to integer division by 2).

    Modular arithmetic is applied in each multiplication and squaring step to
    prevent overflow and to keep numbers small.

  code_snippet: |
    const int M = 1e9 + 7;
    long long modpow(long long a, long long b, long long M) {
        long long res = 1;         // Initialize result as 1 (identity for multiplication)
        a %= M;                    // Reduce 'a' modulo M initially to avoid overflow
        while (b > 0) {            // Loop until exponent becomes 0
            if (b & 1) {           // If current bit of b is 1
                res = (res * a) % M; // Multiply result with current base and take modulo
            }
            a = (a * a) % M;       // Square the base for the next bit
            b >>= 1;               // Shift exponent right by 1 (divide by 2)
        }
        return res;
    }

  step_by_step:
    - Step 1: "Initialize `res = 1` because anything multiplied by 1 remains unchanged."
    - Step 2: "Take `a %= M` to keep numbers manageable from the start."
    - Step 3: "While `b > 0` means we still have bits to process in exponent."
    - Step 4: "`if (b & 1)` checks whether the least significant bit (LSB) of `b` is 1."
    - Step 5: "If LSB is 1, multiply `res` by current base `a` and take modulo M."
    - Step 6: "Square the base (`a = a * a`) to prepare for the next higher bit of b."
    - Step 7: "Reduce base modulo M again to keep values small."
    - Step 8: "Shift `b` right (`b >>= 1`) to process the next bit in the next loop iteration."
    - Step 9: "Repeat until exponent becomes 0, then `res` holds the answer."

  example:
    input: "a = 3, b = 13, M = 1000000007"
    binary_of_b: "13 in binary = 1101"
    dry_run:
      - Start: "res = 1, a = 3, b = 13 (1101)"
      - Iteration 1 (LSB=1): "res = 1*3 % M = 3, a = 3^2 % M = 9, b = 6"
      - Iteration 2 (LSB=0): "res = 3, a = 9^2 % M = 81, b = 3"
      - Iteration 3 (LSB=1): "res = 3*81 % M = 243, a = 81^2 % M = 6561, b = 1"
      - Iteration 4 (LSB=1): "res = 243*6561 % M = 1594323, a = 6561^2 % M = 43046721, b = 0"
    output: "1594323"

29)Smallest Missing Sum Problem:
Explanation:
  We want to find the smallest positive integer that cannot be formed using the sum of some subset of the given array.

  Intuition:
    - Let `ans` denote the smallest number we cannot form so far.
    - Initially `ans = 1` because before using any elements, 1 is the smallest candidate we may miss.
    - After processing some elements, we assume:
        "We can form all sums from 1 up to ans-1."

  Case 1: a[i] <= ans
    - Since we can already form all sums 1..(ans-1), adding a[i] to each gives:
        a[i]+1, a[i]+2, ..., a[i]+(ans-1)
    - This new range is [a[i]+1, ans-1+a[i]].
    - Notice: since a[i] <= ans, the smallest in this new range is a[i]+1 ≤ ans.
      That means it connects directly after ans-1 with no gap.
    - Hence we now cover all sums up to ans-1+a[i].
    - Also, we can form a[i] itself, so indeed coverage is continuous.
    - Therefore, the new possible answer becomes ans = ans + a[i].

  Case 2: a[i] > ans
    - Right now, we can make 1..(ans-1).
    - If we try to add a[i], we get numbers like a[i]+1, a[i]+2,... but since a[i] > ans, this new range starts beyond ans.
    - That means `ans` is still missing and can never be formed because all future numbers are even larger.
    - So the true answer is `ans`.

Formal Proof (Induction Style):
  Base Case:
    - Before taking any elements, we can make nothing. So the smallest missing sum is 1 (ans=1).
  Inductive Hypothesis:
    - Assume after processing first k elements, we can form every number in [1, ans-1].
  Step:
    - If next element a[k] <= ans, then new range [a[k], ans-1+a[k]] merges with [1, ans-1]. So now we can form [1, ans+a[k]-1]. Thus ans updates to ans+a[k].
    - If next element a[k] > ans, then there is a gap at ans which cannot be filled. Hence ans is final answer.
  Therefore, the greedy rule is correct.

Example 1:
  Input: [1,2,5,10]
  Sorted: [1,2,5,10]
  ans=1
  - First element 1 <= 1 → we extend to ans=1+1=2.
  - Next element 2 <= 2 → we extend to ans=2+2=4.
  - Next element 5 > 4 → stop, answer=4.

Example 2:
  Input: [2,3,4]
  Sorted: [2,3,4]
  ans=1
  - First element 2 > 1 → immediately stop, answer=1.

C++ Code:
  #include <bits/stdc++.h>
  using namespace std;
  typedef long long ll;
  int main(){
      ll n;
      cin >> n;
      vector<ll> a(n);
      for(ll i=0;i<n;i++) cin >> a[i];
      sort(a.begin(), a.end());
      ll ans = 1;
      for(ll i=0;i<n;i++){
          if(a[i] <= ans) ans = ans + a[i];
          else break;
      }
      cout << ans << endl;
      return 0;
  }

30)Problem: Longest Mountain in Array
Intuition:
  - A "mountain" is defined as a continuous subarray where:
    - It strictly increases (goes uphill),
    - Reaches a peak,
    - Then strictly decreases (goes downhill).
    - Length of mountain must be at least 3 (since we need left slope, peak, right slope).
  - Example: [2, 1, 4, 7, 3, 2, 5]
    - Mountain found: [1, 4, 7, 3, 2] with length = 5
  - This is **different from Longest Bitonic Subsequence (LBS)** because:
    - LBS allows picking *any elements in order* (not necessarily contiguous).
    - Here, we require **continuous subarrays** only.
    - Hence, LBS needs DP, but this problem can be solved with linear scans.

Approach:
  1. Create two helper arrays:
     - high[i] → length of strictly increasing subarray ending at index i.
     - low[i] → length of strictly decreasing subarray starting at index i.
  2. Fill `high` by scanning left-to-right:
     - if arr[i] > arr[i-1], then high[i] = high[i-1] + 1
     - else reset high[i] = 1
  3. Fill `low` by scanning right-to-left:
     - if arr[i] > arr[i+1], then low[i] = low[i+1] + 1
     - else reset low[i] = 1
  4. Now, check each index i:
     - If high[i] > 1 and low[i] > 1, then arr[i] is a **valid peak**.
     - The mountain length = high[i] + low[i] - 1
       (subtract 1 to avoid counting peak twice).
  5. Take the maximum over all i.

Code Snippet (C++):
  int longestMountain(vector<int>& arr) {
      int n = arr.size();
      vector<int> high(n, 1), low(n, 1);
      // Increasing sequence from left
      for (int i = 1; i < n; i++) {
          if (arr[i] > arr[i - 1]) high[i] = high[i - 1] + 1;
      }
      // Decreasing sequence from right
      for (int i = n - 2; i >= 0; i--) {
          if (arr[i] > arr[i + 1]) low[i] = low[i + 1] + 1;
      }
      int ans = 0;
      for (int i = 0; i < n; i++) {
          if (high[i] > 1 && low[i] > 1) {
              ans = max(ans, high[i] + low[i] - 1);
          }
      }
      return ans;
  }

31)"Construct N as Sum of K Powers of Two":
  problem_statement: |
    Given two integers n and k, determine whether n can be represented as the sum of exactly k powers of two (powers may repeat). If possible, output one such set of k powers of two whose sum is n; otherwise output -1.
  intuition: |
    The binary form of n already gives a sum of powers of 2. The number of set bits in n is the **minimum number** of powers required. Why? Because each set bit corresponds to one power of 2 that cannot be compressed further.
    The **maximum number** of powers possible is n itself (all 1's). So the feasible range for k is:popcount(n) <= k <= n

  reasoning_minimum: |
    - Each set bit contributes one distinct 2^i.
    - You cannot merge different 2^i into a single power-of-two without changing the sum.
    - Therefore, minimum required = number of set bits.

  reasoning_maximum: |
    - The smallest power of two is 1.
    - Splitting every bit until only 1's remain gives exactly n terms.
    - So maximum possible k = n.

  constructive_strategy: |
    - Start with binary decomposition of n (all set bits).
    - While current number of terms < k, split one term into two halves.
    - Each split increases term count by 1, preserves sum.
    - Always split the largest term first (greedy) to keep options open.

  why_priority_queue: |
    - We need to repeatedly access the largest current power.
    - A max-heap (priority_queue in C++) gives O(log M) per operation.
    - This ensures we always split the largest efficiently.

  correctness: |
    - Invariant: sum is preserved (x -> x/2 + x/2).
    - Invariant: count increases by 1 per split.
    - Termination: if k is in [popcount(n), n], we can reach exactly k.

  example_small: |
    n=13, k=5
    - Binary: 13=8+4+1, popcount=3.
    - Valid since 3<=5<=13.
    - Start: {8,4,1}.
    - Split 8 -> {4,4,4,1}.
    - Split 4 -> {4,2,2,4,1} (size=5). Done.

  example_large: |
    n=10^12, k=100
    - popcount ≈ 19.
    - 19 <= 100 <= 10^12, so feasible.
    - Split largest terms until size=100.

  c++_implementation: |
    #include <bits/stdc++.h>
    using namespace std;
    typedef long long ll;
    int main(){
        ll n,k; cin>>n>>k;
        // Count set bits manually
        ll temp=n, popcnt=0;
        while(temp){
            if(temp%2) popcnt++;
            temp/=2;
        }
        if(!(popcnt<=k && k<=n)){
            cout<<-1<<"\n";
            return 0;
        }
        // Build initial heap of powers from binary form
        priority_queue<ll> pq;
        temp=n; ll idx=0;
        while(temp){
            if(temp%2) pq.push(1LL<<idx);
            idx++; temp/=2;
        }
        // Split until we have exactly k terms
        while((ll)pq.size()<k && pq.top()>1){
            ll num=pq.top(); pq.pop();
            pq.push(num/2);
            pq.push(num/2);
        }
        // Output terms
        vector<ll> res;
        while(!pq.empty()){res.push_back(pq.top()); pq.pop();}
        for(ll x:res) cout<<x<<" ";
        cout<<"\n";
    }
  note_on_greedy: |
    Splitting the largest term first maximizes flexibility: large terms can still be split many times, while small ones (like 1) cannot be split at all. Thus the greedy choice avoids dead ends.

32)Problem: "Kth Smallest Element in a Sorted Matrix"
Explanation:
  Intuition:
    - We are given a matrix where:
        * Each row is sorted in increasing order (left → right).
        * Each column is sorted in increasing order (top → bottom).
    - We want the kth smallest element in the entire matrix.
    - A brute force approach would be:
        * Flatten all elements into a single array.
        * Sort the array.
        * Take the kth element.
      But this is inefficient (O(m*n log(m*n)) time).
    - Instead, we use a **min-heap (priority queue)** to extract elements in sorted order efficiently.

  Why Min-Heap?:
    - A min-heap always gives us the current smallest element at the top.
    - Since the matrix rows are sorted:
        * If we take one element from a row, the **next smallest candidate** from that row is the element immediately to its right.
    - By repeatedly popping the smallest element and pushing the next candidate from the same row, we generate numbers in sorted order until we reach the kth one.

  Stored in Heap:
    - Each heap entry is a triplet: [value, row_index, col_index].
    - Why?
        * `value` → Needed to compare and decide the smallest.
        * `row_index` → To know which row this value came from.
        * `col_index` → To know where we are in that row, so we can push the "next element" in the same row.

  Step-by-Step Process:
    1. Initialize the heap:
       - Push the first element of every row (matrix[i][0]) into the heap.
       - Each push is of form: [matrix[i][0], i, 0].
       - Example: if matrix = [[1,5,9],[10,11,13],[12,13,15]], heap initially has:
           [1,0,0], [10,1,0], [12,2,0].
    2. Repeatedly extract the smallest element (heap top).
       - Each time we pop, that element is the next smallest in the entire matrix.
       - Store it into "ans".
    3. After popping an element from row r and col c:
       - If col+1 < n, push the next element in the same row:
           [matrix[r][c+1], r, c+1].
       - This works because rows are sorted, so the next smallest candidate from that row is at (r, c+1).
    4. Stop after extracting k elements. The kth popped element is the answer.

  Example Walkthrough:
    Matrix:
      [ [1,  5,  9],
        [10, 11, 13],
        [12, 13, 15] ]
    k = 6

    Initial Heap:
      - [1,0,0], [10,1,0], [12,2,0]

    Iterations:
      - Pop1: [1,0,0] → ans = [1]
        Push [5,0,1]
      - Pop2: [5,0,1] → ans = [1,5]
        Push [9,0,2]
      - Pop3: [9,0,2] → ans = [1,5,9]
        (no push, since col=2 is last column)
      - Pop4: [10,1,0] → ans = [1,5,9,10]
        Push [11,1,1]
      - Pop5: [11,1,1] → ans = [1,5,9,10,11]
        Push [13,1,2]
      - Pop6: [12,2,0] → ans = [1,5,9,10,11,12]
        ✅ 6th smallest is 12

  Why Only Push Next Element in Same Row:
    - After we pop an element [val, r, c]:
        * matrix[r][c+1] is guaranteed to be ≥ val (since row is sorted).
        * matrix[r+1][c] is not directly considered here, because it was already inserted initially when we added row0’s first element, row1’s first element, etc.
    - Thus, to maintain order, we only advance along rows when necessary.

Code-with-Comments:
  class Solution {
  public:
      int kthSmallest(vector<vector<int>>& matrix, int k) {
          int m = matrix.size(), n = matrix[0].size();
          // Min-heap storing [value, row, col]
          priority_queue<vector<int>, vector<vector<int>>, greater<vector<int>>> pq;
          // Step 1: Push the first element of every row into heap
          for (int i = 0; i < m; i++) {
              pq.push({matrix[i][0], i, 0});
          }
          // Step 2: Extract elements in increasing order
          vector<int> ans;
          while (ans.size() < k) {
              // Top element is always the smallest currently available
              int num = pq.top()[0], r = pq.top()[1], c = pq.top()[2];
              ans.push_back(num);
              pq.pop();
              // Step 3: Push next element in the same row, if it exists
              if (c + 1 < n) {
                  pq.push({matrix[r][c+1], r, c+1});
              }
          }
          // Step 4: kth element is the answer
          return ans[k-1];
      }
  };

33)Modulo_Pair_Counting_Comparison:
  concept_1:
    name: "Modulo Pair Counting (Allowing Index Reuse)"
    description: |
      In this method, we count all pairs (i,j) such that (a[i] + a[j]) % k == 0,
      and we allow the **same element (index) to participate in multiple pairs**.
      This is like "all possible valid combinations" counting.
    approach:
      - Iterate through the array.
      - Compute remainder rem = a[i] % k.
      - Compute required remainder req = (k - rem) % k.
      - Check if there are any previous elements with remainder req:
          - If yes, each occurrence can form a valid pair with current element.
          - Add number of occurrences to answer.
      - Update map with current remainder.
    characteristics:
      - An element can appear in multiple pairs.
      - Simple to implement using map of frequencies.
      - Counts all valid combinations.
    time_complexity: O(n)
    space_complexity: O(k)
    dry_run_example:
      input_array: [1, 2, 2, 3, 2, 4, 10]
      k: 2
      step_by_step:
        - i=0, a[i]=1, rem=1, req=1, map={}, ans=0 => update map={1:1}
        - i=1, a[i]=2, rem=0, req=0, map={1:1}, ans=0 => update map={1:1,0:1}
        - i=2, a[i]=2, rem=0, req=0, map={1:1,0:1}, ans=0+1=1 => update map={1:1,0:2}
        - i=3, a[i]=3, rem=1, req=1, map={1:1,0:2}, ans=1+1=2 => update map={1:2,0:2}
        - i=4, a[i]=2, rem=0, req=0, map={1:2,0:2}, ans=2+2=4 => update map={1:2,0:3}
        - i=5, a[i]=4, rem=0, req=0, map={1:2,0:3}, ans=4+3=7 => update map={1:2,0:4}
        - i=6, a[i]=10, rem=0, req=0, map={1:2,0:4}, ans=7+4=11 => update map={1:2,0:5}
      total_pairs_counted: 11
    code: |
      #include <bits/stdc++.h>
      using namespace std;
      typedef long long ll;
      int main() {
          ll n,k;
          cin>>n>>k;
          vector<ll>a(n);
          for(ll i=0;i<n;i++) cin>>a[i];
          map<ll,ll> mp;
          ll ans=0;
          for(ll i=0;i<n;i++){
              ll rem = a[i] % k;
              ll req = (k - rem) % k;
              if(mp.find(req)!=mp.end()){
                  ans += mp[req];
              }
              mp[rem]++;
          }
          cout<<ans<<"\n";
          return 0;
      }

  concept_2:
    name: "Modulo Pair Counting (Distinct Index Pairs)"
    description: |
      In this method, we count pairs (i,j) such that (a[i] + a[j]) % k == 0,
      but **each index is used at most once**. This ensures no element appears in more than one pair.
    approach:
      - Count frequencies of each remainder modulo k.
      - For remainder r:
          - Find complement = (k - r) % k
          - If r == complement (special case: 0 or k/2), pairs = freq[r]//2
          - Else, to avoid double counting, only process when r < complement: pairs = min(freq[r], freq[complement])
      - Sum pairs from all groups.
    characteristics:
      - Each element is used in **at most one pair**.
      - Pairing is done **between remainder groups** or within group for special cases.
      - Output is always ≤ Concept 1.
    time_complexity: O(n+k)
    space_complexity: O(k)
    dry_run_example:
      input_array: [1, 2, 2, 3, 2, 4, 10]
      k: 2
      remainder_count: {0:5, 1:2}
      step_by_step:
        - r=0, complement=0, special case => pairs = 5//2=2
        - r=1, complement=1, special case => pairs = 2//2=1
      total_pairs_counted: 3
      pairs_listed: [(0,3),(1,2),(4,5)]
    code: |
      #include <bits/stdc++.h>
      using namespace std;
      typedef long long ll;
      int main() {
          ll n,k;
          cin>>n>>k;
          vector<ll>a(n);
          for(ll i=0;i<n;i++) cin>>a[i];
          map<ll,ll> mp;
          for(ll i=0;i<n;i++){
              mp[a[i]%k]++;
          }
          ll ans=0;
          for(ll r=0;r<k;r++){
              ll complement=(k-r)%k;
              if(r==complement){
                  ans += mp[r]/2;
              } else if(r<complement){
                  ans += min(mp[r], mp[complement]);
              }
          }
          cout<<ans<<"\n";
          return 0;
      }

comparison:
  main_difference: |
    Concept 1 allows the **same element to be part of multiple pairs**, 
    whereas Concept 2 ensures **each element is used at most once**.
  implication: |
    - Concept 1 may overcount because it includes overlapping pairs.
    - Concept 2 gives the maximum number of **non-overlapping pairs**.
  output_example:
    input_array: [1,2,2,3,2,4,10], k=2
    concept_1_output: 11
    concept_2_output: 3
  when_to_use:
    - Concept 1: When all combinations matter, index reuse allowed.
    - Concept 2: When each element can be paired **at most once**, like forming distinct sets or matches.

34)Problem: Maximum Frequency of Integer Ratios (Skipping Division by Zero)
Intuition:
  The goal is to find the most frequent ratio b[i]/a[i] among integer pairs (a[i], b[i]).
  Instead of using floating-point division, which can cause rounding issues,
  we use **reduced integer pairs** to represent each ratio uniquely.

  Using doubles:
    - (1/3) and (2/6) might not compare equal due to precision errors.
    - Floating-point operations are slower and unsafe for large integers.

  So we use integer pairs:
    - Reduce (a[i], b[i]) by dividing both by their gcd.
    - This ensures that equivalent ratios like (2, -4) and (1, -2) are treated as identical.

  Sign normalization:
    - If a[i] < 0, multiply both a[i] and b[i] by -1.
    - This ensures denominator (a[i]) is always positive.
    - Thus, (-1, 2) and (1, -2) are treated as the same ratio.

Approach:
  1. Read n and two arrays a and b.
  2. For each i:
      - If a[i] == 0, skip (division by zero is invalid).
      - Otherwise:
          * Compute gcd(|a[i]|, |b[i]|).
          * Divide both a[i] and b[i] by gcd.
          * Normalize sign: if a[i] < 0, flip both signs.
          * Increment count of pair (a[i], b[i]).
  3. The maximum frequency among all pairs is our result.

Key Points:
  - gcd() ensures ratios are reduced to their simplest form.
  - Skipping division by zero avoids undefined ratios.
  - Using `map<pair<ll,ll>, ll>` ensures each ratio is tracked efficiently.

Time Complexity:
  O(n log M), where M is the maximum magnitude of numbers (for gcd).

Space Complexity:
  O(n), for storing frequency map.

C++ Code:
  #include <bits/stdc++.h>
  using namespace std;
  typedef long long ll;
  int main() {
      ll n;
      cin >> n;
      vector<ll> a(n), b(n);
      for (ll i = 0; i < n; i++) cin >> a[i];
      for (ll i = 0; i < n; i++) cin >> b[i];
      map<pair<ll, ll>, ll> freq;
      for (ll i = 0; i < n; i++) {
          if (a[i] == 0) continue; // skip division by zero
          ll g = __gcd(abs(a[i]), abs(b[i]));
          a[i] /= g;
          b[i] /= g;
          // Normalize sign to keep denominator positive
          if (a[i] < 0) {
              a[i] = -a[i];
              b[i] = -b[i];
          }
          freq[{a[i], b[i]}]++;
      }
      ll maxi = 0;
      for (auto &p : freq)maxi = max(maxi, p.second);
      cout << maxi << "\n";
      return 0;
  }
